{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The os module has a perfect method to list files in a directory.\n",
    "- Pandas json normalize could work here but is not necessary to convert the JSON data to a dataframe.\n",
    "- You may need a nested for-loop to access each sale!\n",
    "- We've put a lot of time into creating the structure of this repository, and it's a good example for future projects.  In the file functions_variables.py, there is an example function that you can import and use.  If you have any variables, functions or classes that you want to make, they can be put in the functions_variables.py file and imported into a notebook.  Note that only .py files can be imported into a notebook. If you want to import everything from a .py file, you can use the following:\n",
    "```python\n",
    "from functions_variables import *\n",
    "```\n",
    "If you just import functions_variables, then each object from the file will need to be prepended with \"functions_variables\"\\\n",
    "Using this .py file will keep your notebooks very organized and make it easier to reuse code between notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (this is not an exhaustive list of libraries)\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from functions_variables import encode_tags\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".gitkeep\n",
      "AK_Juneau_0.json\n",
      "AK_Juneau_1.json\n",
      "AK_Juneau_2.json\n",
      "AK_Juneau_3.json\n",
      "AK_Juneau_4.json\n",
      "AL_Montgomery_0.json\n",
      "AL_Montgomery_1.json\n",
      "AL_Montgomery_2.json\n",
      "AL_Montgomery_3.json\n",
      "AL_Montgomery_4.json\n",
      "AR_LittleRock_0.json\n",
      "AR_LittleRock_1.json\n",
      "AR_LittleRock_2.json\n",
      "AR_LittleRock_3.json\n",
      "AR_LittleRock_4.json\n",
      "AZ_Phoenix_0.json\n",
      "AZ_Phoenix_1.json\n",
      "AZ_Phoenix_2.json\n",
      "AZ_Phoenix_3.json\n",
      "AZ_Phoenix_4.json\n",
      "CA_Sacramento_0.json\n",
      "CA_Sacramento_1.json\n",
      "CA_Sacramento_2.json\n",
      "CA_Sacramento_3.json\n",
      "CA_Sacramento_4.json\n",
      "CO_Denver_0.json\n",
      "CO_Denver_1.json\n",
      "CO_Denver_2.json\n",
      "CO_Denver_3.json\n",
      "CO_Denver_4.json\n",
      "CT_Hartford_0.json\n",
      "CT_Hartford_1.json\n",
      "CT_Hartford_2.json\n",
      "CT_Hartford_3.json\n",
      "CT_Hartford_4.json\n",
      "DE_Dover_0.json\n",
      "DE_Dover_1.json\n",
      "DE_Dover_2.json\n",
      "DE_Dover_3.json\n",
      "DE_Dover_4.json\n",
      "FL_Tallahassee_0.json\n",
      "FL_Tallahassee_1.json\n",
      "FL_Tallahassee_2.json\n",
      "FL_Tallahassee_3.json\n",
      "FL_Tallahassee_4.json\n",
      "GA_Atlanta_0.json\n",
      "GA_Atlanta_1.json\n",
      "GA_Atlanta_2.json\n",
      "GA_Atlanta_3.json\n",
      "GA_Atlanta_4.json\n",
      "HI_Honolulu_0.json\n",
      "HI_Honolulu_1.json\n",
      "HI_Honolulu_2.json\n",
      "HI_Honolulu_3.json\n",
      "HI_Honolulu_4.json\n",
      "IA_DesMoines_0.json\n",
      "IA_DesMoines_1.json\n",
      "IA_DesMoines_2.json\n",
      "IA_DesMoines_3.json\n",
      "IA_DesMoines_4.json\n",
      "ID_Boise_0.json\n",
      "ID_Boise_1.json\n",
      "ID_Boise_2.json\n",
      "ID_Boise_3.json\n",
      "ID_Boise_4.json\n",
      "IL_Springfield_0.json\n",
      "IL_Springfield_1.json\n",
      "IL_Springfield_2.json\n",
      "IL_Springfield_3.json\n",
      "IL_Springfield_4.json\n",
      "IN_Indianapolis_0.json\n",
      "IN_Indianapolis_1.json\n",
      "IN_Indianapolis_2.json\n",
      "IN_Indianapolis_3.json\n",
      "IN_Indianapolis_4.json\n",
      "KS_Topeka_0.json\n",
      "KS_Topeka_1.json\n",
      "KS_Topeka_2.json\n",
      "KS_Topeka_3.json\n",
      "KS_Topeka_4.json\n",
      "KY_Frankfort_0.json\n",
      "KY_Frankfort_1.json\n",
      "KY_Frankfort_2.json\n",
      "KY_Frankfort_3.json\n",
      "KY_Frankfort_4.json\n",
      "LA_BatonRouge_0.json\n",
      "LA_BatonRouge_1.json\n",
      "LA_BatonRouge_2.json\n",
      "LA_BatonRouge_3.json\n",
      "LA_BatonRouge_4.json\n",
      "MA_Boston_0.json\n",
      "MA_Boston_1.json\n",
      "MA_Boston_2.json\n",
      "MA_Boston_3.json\n",
      "MA_Boston_4.json\n",
      "MD_Annapolis_0.json\n",
      "MD_Annapolis_1.json\n",
      "MD_Annapolis_2.json\n",
      "MD_Annapolis_3.json\n",
      "MD_Annapolis_4.json\n",
      "ME_Augusta_0.json\n",
      "ME_Augusta_1.json\n",
      "ME_Augusta_2.json\n",
      "ME_Augusta_3.json\n",
      "ME_Augusta_4.json\n",
      "MI_Lansing_0.json\n",
      "MI_Lansing_1.json\n",
      "MI_Lansing_2.json\n",
      "MI_Lansing_3.json\n",
      "MI_Lansing_4.json\n",
      "MN_St.Paul_0.json\n",
      "MN_St.Paul_1.json\n",
      "MN_St.Paul_2.json\n",
      "MN_St.Paul_3.json\n",
      "MN_St.Paul_4.json\n",
      "MO_JeffersonCity_0.json\n",
      "MO_JeffersonCity_1.json\n",
      "MO_JeffersonCity_2.json\n",
      "MO_JeffersonCity_3.json\n",
      "MO_JeffersonCity_4.json\n",
      "MS_Jackson_0.json\n",
      "MS_Jackson_1.json\n",
      "MS_Jackson_2.json\n",
      "MS_Jackson_3.json\n",
      "MS_Jackson_4.json\n",
      "MT_Helena_0.json\n",
      "MT_Helena_1.json\n",
      "MT_Helena_2.json\n",
      "MT_Helena_3.json\n",
      "MT_Helena_4.json\n",
      "NC_Raleigh_0.json\n",
      "NC_Raleigh_1.json\n",
      "NC_Raleigh_2.json\n",
      "NC_Raleigh_3.json\n",
      "NC_Raleigh_4.json\n",
      "ND_Bismarck_0.json\n",
      "ND_Bismarck_1.json\n",
      "ND_Bismarck_2.json\n",
      "ND_Bismarck_3.json\n",
      "ND_Bismarck_4.json\n",
      "NE_Lincoln_0.json\n",
      "NE_Lincoln_1.json\n",
      "NE_Lincoln_2.json\n",
      "NE_Lincoln_3.json\n",
      "NE_Lincoln_4.json\n",
      "NH_Concord_0.json\n",
      "NH_Concord_1.json\n",
      "NH_Concord_2.json\n",
      "NH_Concord_3.json\n",
      "NH_Concord_4.json\n",
      "NJ_Trenton_0.json\n",
      "NJ_Trenton_1.json\n",
      "NJ_Trenton_2.json\n",
      "NJ_Trenton_3.json\n",
      "NJ_Trenton_4.json\n",
      "NM_SantaFe_0.json\n",
      "NM_SantaFe_1.json\n",
      "NM_SantaFe_2.json\n",
      "NM_SantaFe_3.json\n",
      "NM_SantaFe_4.json\n",
      "NV_CarsonCity_0.json\n",
      "NV_CarsonCity_1.json\n",
      "NV_CarsonCity_2.json\n",
      "NV_CarsonCity_3.json\n",
      "NV_CarsonCity_4.json\n",
      "NY_Albany_0.json\n",
      "NY_Albany_1.json\n",
      "NY_Albany_2.json\n",
      "NY_Albany_3.json\n",
      "NY_Albany_4.json\n",
      "OH_Columbus_0.json\n",
      "OH_Columbus_1.json\n",
      "OH_Columbus_2.json\n",
      "OH_Columbus_3.json\n",
      "OH_Columbus_4.json\n",
      "OK_OklahomaCity_0.json\n",
      "OK_OklahomaCity_1.json\n",
      "OK_OklahomaCity_2.json\n",
      "OK_OklahomaCity_3.json\n",
      "OK_OklahomaCity_4.json\n",
      "OR_Salem_0.json\n",
      "OR_Salem_1.json\n",
      "OR_Salem_2.json\n",
      "OR_Salem_3.json\n",
      "OR_Salem_4.json\n",
      "PA_Harrisburg_0.json\n",
      "PA_Harrisburg_1.json\n",
      "PA_Harrisburg_2.json\n",
      "PA_Harrisburg_3.json\n",
      "PA_Harrisburg_4.json\n",
      "processed\n",
      "RI_Providence_0.json\n",
      "RI_Providence_1.json\n",
      "RI_Providence_2.json\n",
      "RI_Providence_3.json\n",
      "RI_Providence_4.json\n",
      "SC_Columbia_0.json\n",
      "SC_Columbia_1.json\n",
      "SC_Columbia_2.json\n",
      "SC_Columbia_3.json\n",
      "SC_Columbia_4.json\n",
      "SD_Pierre_0.json\n",
      "SD_Pierre_1.json\n",
      "SD_Pierre_2.json\n",
      "SD_Pierre_3.json\n",
      "SD_Pierre_4.json\n",
      "TN_Nashville_0.json\n",
      "TN_Nashville_1.json\n",
      "TN_Nashville_2.json\n",
      "TN_Nashville_3.json\n",
      "TN_Nashville_4.json\n",
      "TX_Austin_0.json\n",
      "TX_Austin_1.json\n",
      "TX_Austin_2.json\n",
      "TX_Austin_3.json\n",
      "TX_Austin_4.json\n",
      "UT_SaltLakeCity_0.json\n",
      "UT_SaltLakeCity_1.json\n",
      "UT_SaltLakeCity_2.json\n",
      "UT_SaltLakeCity_3.json\n",
      "UT_SaltLakeCity_4.json\n",
      "VA_Richmond_0.json\n",
      "VA_Richmond_1.json\n",
      "VA_Richmond_2.json\n",
      "VA_Richmond_3.json\n",
      "VA_Richmond_4.json\n",
      "VT_Montpelier_0.json\n",
      "VT_Montpelier_1.json\n",
      "VT_Montpelier_2.json\n",
      "VT_Montpelier_3.json\n",
      "VT_Montpelier_4.json\n",
      "WA_Olympia_0.json\n",
      "WA_Olympia_1.json\n",
      "WA_Olympia_2.json\n",
      "WA_Olympia_3.json\n",
      "WA_Olympia_4.json\n",
      "WI_Madison_0.json\n",
      "WI_Madison_1.json\n",
      "WI_Madison_2.json\n",
      "WI_Madison_3.json\n",
      "WI_Madison_4.json\n",
      "WV_Charleston_0.json\n",
      "WV_Charleston_1.json\n",
      "WV_Charleston_2.json\n",
      "WV_Charleston_3.json\n",
      "WV_Charleston_4.json\n",
      "WY_Cheyenne_0.json\n",
      "WY_Cheyenne_1.json\n",
      "WY_Cheyenne_2.json\n",
      "WY_Cheyenne_3.json\n",
      "WY_Cheyenne_4.json\n"
     ]
    }
   ],
   "source": [
    "# load one file first to see what type of data you're dealing with and what attributes it has\n",
    "\n",
    "path = r\"C:\\Users\\turab\\data-project-midterm\\data\"\n",
    "\n",
    "# Use os.listdir to get files\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Print all files\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all sales into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\turab\\AppData\\Local\\Temp\\ipykernel_5512\\2437266733.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(data_frames, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       last_update_date                                               tags  \\\n",
      "0  2023-09-19T20:52:50Z  [carport, community_outdoor_space, cul_de_sac,...   \n",
      "1                  None                                               None   \n",
      "2                  None                                               None   \n",
      "3                  None                                               None   \n",
      "4                  None                                               None   \n",
      "\n",
      "   ... community.description.name location.county  \n",
      "0  ...                        NaN             NaN  \n",
      "1  ...                        NaN             NaN  \n",
      "2  ...                        NaN             NaN  \n",
      "3  ...                        NaN             NaN  \n",
      "4  ...                        NaN             NaN  \n",
      "\n",
      "[5 rows x 67 columns]\n"
     ]
    }
   ],
   "source": [
    "def load_and_combine_json_files(directory):\n",
    "    data_frames = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "                \n",
    "                # Normalize the JSON structure to a flat table\n",
    "                if 'data' in json_data and 'results' in json_data['data']:\n",
    "                    df = json_normalize(json_data['data']['results'])\n",
    "                    data_frames.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into one\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "directory = r\"C:\\Users\\turab\\data-project-midterm\\data\"  # Update this path\n",
    "\n",
    "# Load and combine the JSON files\n",
    "combined_data = load_and_combine_json_files(directory)\n",
    "\n",
    "print(combined_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, ensure that you have all sales in a dataframe.\n",
    "- Is each cell one value, or do some cells have lists?\n",
    "- Maybe the \"tags\" will help create some features.\n",
    "- What are the data types of each column?\n",
    "- Some sales may not actually include the sale price.  These rows should be dropped.\n",
    "- Some sales don't include the property type.\n",
    "- There are a lot of None values.  Should these be dropped or replaced with something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop or replace values as neccesary \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'last_update_date': False, 'tags': True, 'permalink': False, 'status': False, 'list_date': False, 'open_houses': False, 'branding': True, 'list_price': False, 'property_id': False, 'photos': True, 'community': False, 'virtual_tours': True, 'listing_id': False, 'price_reduced_amount': False, 'matterport': False, 'primary_photo.href': False, 'source.plan_id': False, 'source.agents': True, 'source.spec_id': False, 'source.type': False, 'description.year_built': False, 'description.baths_3qtr': False, 'description.sold_date': False, 'description.sold_price': False, 'description.baths_full': False, 'description.name': False, 'description.baths_half': False, 'description.lot_sqft': False, 'description.sqft': False, 'description.baths': False, 'description.sub_type': False, 'description.baths_1qtr': False, 'description.garage': False, 'description.stories': False, 'description.beds': False, 'description.type': False, 'lead_attributes.show_contact_an_agent': False, 'flags.is_new_construction': False, 'flags.is_for_rent': False, 'flags.is_subdivision': False, 'flags.is_contingent': False, 'flags.is_price_reduced': False, 'flags.is_pending': False, 'flags.is_foreclosure': False, 'flags.is_plan': False, 'flags.is_coming_soon': False, 'flags.is_new_listing': False, 'products.brand_name': False, 'other_listings.rdc': True, 'location.address.postal_code': False, 'location.address.state': False, 'location.address.coordinate.lon': False, 'location.address.coordinate.lat': False, 'location.address.city': False, 'location.address.state_code': False, 'location.address.line': False, 'location.street_view_url': False, 'location.county.fips_code': False, 'location.county.name': False, 'primary_photo': False, 'source': False, 'products': False, 'location.address.coordinate': False, 'other_listings': False, 'community.advertisers': True, 'community.description.name': False, 'location.county': False}\n"
     ]
    }
   ],
   "source": [
    "# Check if some columns have lists\n",
    "\n",
    "list_columns_combined_data = check_for_lists(combined_data)\n",
    "print(list_columns_combined_data)\n",
    "\n",
    "# Tags, branding, photos, virtual tours, source agents, other listings, etc have lists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tags:\n",
      "clubhouse\n",
      "community_clubhouse\n",
      "jack_and_jill_bathroom\n",
      "basketball_court\n",
      "wrap_around_porch\n",
      "courtyard_entry\n",
      "library\n",
      "fenced_yard\n",
      "carport\n",
      "lake\n",
      "garage_2_or_more\n",
      "lake_view\n",
      "trails\n",
      "baseball\n",
      "garage_3_or_more\n",
      "community_elevator\n",
      "tennis\n",
      "boat_dock\n",
      "pond\n",
      "single_story\n",
      "park\n",
      "laundry_room\n",
      "hill_or_mountain_view\n",
      "garage_1_or_more\n",
      "community_center\n",
      "community_park\n",
      "forced_air\n",
      "wine_cellar\n",
      "hardwood_floors\n",
      "two_or_more_stories\n",
      "golf_course_lot_or_frontage\n",
      "fixer_upper\n",
      "first_floor_master_bedroom\n",
      "indoor_basketball_court\n",
      "high_ceiling\n",
      "master_bathroom\n",
      "central_air\n",
      "hoa\n",
      "outdoor_kitchen\n",
      "mountain_view\n",
      "views\n",
      "pets_allowed\n",
      "community_horse_facilities\n",
      "large_porch\n",
      "city_view\n",
      "ranch\n",
      "marina\n",
      "community_spa_or_hot_tub\n",
      "community_outdoor_space\n",
      "family_room\n",
      "water_view\n",
      "private_backyard\n",
      "beach\n",
      "disability_features\n",
      "ensuite\n",
      "ocean_view\n",
      "smart_homes\n",
      "community_swimming_pool\n",
      "outbuilding\n",
      "efficient\n",
      "community_tennis_court\n",
      "central_heat\n",
      "fruit_trees\n",
      "guest_parking\n",
      "white_kitchen\n",
      "furniture\n",
      "den_or_office\n",
      "open_house\n",
      "master_bedroom\n",
      "guest_house\n",
      "updated_kitchen\n",
      "dual_master_bedroom\n",
      "senior_community\n",
      "well_water\n",
      "washer_dryer\n",
      "basketball\n",
      "media_room\n",
      "river_access\n",
      "handicap_access\n",
      "gated_community\n",
      "maintenance\n",
      "kitchen_island\n",
      "theater_room\n",
      "beautiful_backyard\n",
      "volleyball\n",
      "golf_course_view\n",
      "elevator\n",
      "open_floor_plan\n",
      "front_porch\n",
      "farm\n",
      "gourmet_kitchen\n",
      "soccer\n",
      "view\n",
      "exposed_brick\n",
      "medicalcare\n",
      "playground\n",
      "rental_property\n",
      "detached_guest_house\n",
      "two_master_suites\n",
      "two_kitchen\n",
      "waterfront\n",
      "basement\n",
      "master_suite\n",
      "energy_efficient\n",
      "game_room\n",
      "greenhouse\n",
      "recreation_facilities\n",
      "spa_or_hot_tub\n",
      "floor_plan\n",
      "solar_system\n",
      "groundscare\n",
      "swimming_pool\n",
      "rv_or_boat_parking\n",
      "community_boat_facilities\n",
      "big_yard\n",
      "solar_panels\n",
      "shopping\n",
      "no_hoa\n",
      "rv_parking\n",
      "private_courtyard\n",
      "river_view\n",
      "dining_room\n",
      "corner_lot\n",
      "granite_kitchen\n",
      "large_kitchen\n",
      "modern_kitchen\n",
      "dishwasher\n",
      "wooded_land\n",
      "cathedral_ceiling\n",
      "open_kitchen\n",
      "security\n",
      "community_security_features\n",
      "investment_opportunity\n",
      "greenbelt\n",
      "community_gym\n",
      "fireplace\n",
      "storm_shelter\n",
      "golf_course\n",
      "new_roof\n",
      "coffer_ceiling\n",
      "low_hoa\n",
      "tennis_court\n",
      "big_lot\n",
      "vaulted_ceiling\n",
      "community_golf\n",
      "private_bathroom\n",
      "horse_facilities\n",
      "private_parking\n",
      "fenced_courtyard\n",
      "screen_porch\n",
      "cul_de_sac\n",
      "big_bathroom\n"
     ]
    }
   ],
   "source": [
    "# Display all from tags column \n",
    "\n",
    "if 'tags' in combined_data.columns:\n",
    "    # Initialize an empty set to store unique tags\n",
    "    unique_tags = set()\n",
    "    \n",
    "    # Iterate over each entry in the 'tags' column\n",
    "    for tag_list in combined_data['tags']:\n",
    "        if isinstance(tag_list, list):  # Ensure the entry is a list\n",
    "            unique_tags.update(tag_list)  # Add tags to the set, avoiding duplicates\n",
    "    \n",
    "    # Print the unique set of tags\n",
    "    print(\"Unique tags:\")\n",
    "    for tag in unique_tags:\n",
    "        print(tag)\n",
    "else:\n",
    "    print(\"'tags' column does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column:\n",
      "[dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('float64'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('float64'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('float64'), dtype('float64'), dtype('O'), dtype('float64'), dtype('float64'), dtype('O'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('O'), dtype('O'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('float64'), dtype('float64'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('float64')]\n"
     ]
    }
   ],
   "source": [
    "# Data types of each column\n",
    "\n",
    "data_types_list = combined_data.dtypes.tolist()\n",
    "\n",
    "print(\"Data types of each column:\")\n",
    "print(data_types_list)\n",
    "\n",
    "# There are objects, float64's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame after dropping rows with missing sales price: (6716, 67)\n"
     ]
    }
   ],
   "source": [
    "# drop sales that dont include sales price\n",
    "\n",
    "combined_data = combined_data.dropna(subset=['description.sold_price'])\n",
    "\n",
    "# Print the shape of the DataFrame to verify the number of rows after dropping\n",
    "print(\"Shape of DataFrame after dropping rows with missing sales price:\", combined_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame after dropping rows with missing property type and 'None': (1172, 67)\n"
     ]
    }
   ],
   "source": [
    "# Drop sales that dont have a property type\n",
    "# Display the property type column\n",
    "\n",
    "df_filtered = combined_data.dropna(subset=['description.sub_type'])\n",
    "df_filtered = df_filtered[df_filtered['description.sub_type'].str.lower() != 'none']\n",
    "\n",
    "# Print the shape of the DataFrame to verify the number of rows after dropping\n",
    "print(\"Shape of DataFrame after dropping rows with missing property type and 'None':\", df_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the fact that with tags, there are a lot of categorical variables.\n",
    "- How many columns would we have if we OHE tags, city and state?\n",
    "- Perhaps we can get rid of tags that have a low frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['last_update_date', 'tags', 'permalink', 'status', 'list_date',\n",
      "       'open_houses', 'branding', 'property_id', 'photos', 'community',\n",
      "       'virtual_tours', 'listing_id', 'matterport', 'primary_photo.href',\n",
      "       'source.plan_id', 'source.agents', 'source.spec_id', 'source.type',\n",
      "       'description.sold_date', 'description.name', 'description.sub_type',\n",
      "       'description.baths_1qtr', 'description.type',\n",
      "       'lead_attributes.show_contact_an_agent', 'flags.is_new_construction',\n",
      "       'flags.is_for_rent', 'flags.is_subdivision', 'flags.is_contingent',\n",
      "       'flags.is_price_reduced', 'flags.is_pending', 'flags.is_foreclosure',\n",
      "       'flags.is_plan', 'flags.is_coming_soon', 'flags.is_new_listing',\n",
      "       'products.brand_name', 'other_listings.rdc',\n",
      "       'location.address.postal_code', 'location.address.state',\n",
      "       'location.address.city', 'location.address.state_code',\n",
      "       'location.address.line', 'location.street_view_url',\n",
      "       'location.county.fips_code', 'location.county.name', 'primary_photo',\n",
      "       'source', 'products', 'location.address.coordinate', 'other_listings',\n",
      "       'community.advertisers', 'community.description.name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# See what categorical columns exist including lists or dicts\n",
    "\n",
    "categorical_columns = df_filtered.select_dtypes(include=['object']).columns\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove photos column\n",
    "\n",
    "df_filtered = df_filtered.drop(columns=['photos'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the lists in the 'tags' column\n",
    "df_filtered['tags'] = df_filtered['tags'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Perform one-hot encoding on the flattened 'tags' column\n",
    "\n",
    "df_filtered = pd.get_dummies(df_filtered, columns=['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the 'permalink' column\n",
    "\n",
    "df_filtered['permalink_encoded'] = label_encoder.fit_transform(df_filtered['permalink'])\n",
    "\n",
    "# Drop the original 'permalink' column\n",
    "\n",
    "df_filtered.drop(columns=['permalink'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'sold' column\n",
    "\n",
    "status_onehot = pd.get_dummies(df_filtered['status'], prefix='status')\n",
    "\n",
    "df_filtered = pd.concat([df_filtered, status_onehot], axis=1)\n",
    "\n",
    "# Drop the original 'status' column\n",
    "df_filtered.drop(columns=['status'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionaries to strings in the 'branding' column\n",
    "df_filtered['branding'] = df_filtered['branding'].apply(lambda x: str(x))\n",
    "\n",
    "# Perform one-hot encoding on the 'branding' column\n",
    "one_hot_encoded_branding = pd.get_dummies(df_filtered['branding'], prefix='branding')\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "df_filtered = pd.concat([df_filtered, one_hot_encoded_branding], axis=1)\n",
    "\n",
    "# Drop the original 'branding' column\n",
    "df_filtered.drop(columns=['branding'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace negative placeholder values with None\n",
    "\n",
    "df_filtered = df_filtered.replace(-9223372037, np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten lists and dictionaries\n",
    "\n",
    "def flatten_value(x):\n",
    "    if isinstance(x, list):\n",
    "        return ','.join(map(str, x))\n",
    "    elif isinstance(x, dict):\n",
    "        return ','.join(str(v) for v in x.values())\n",
    "    else:\n",
    "        return str(x) if x is not None else ''\n",
    "\n",
    "# Flatten lists or dictionaries in all columns\n",
    "\n",
    "for col in df_filtered.columns:\n",
    "    df_filtered[col] = df_filtered[col].apply(flatten_value)\n",
    "\n",
    "# Convert datetime strings to Unix timestamps (integers)\n",
    "\n",
    "datetime_columns = ['last_update_date', 'list_date', 'description.sold_date']\n",
    "for col in datetime_columns:\n",
    "    df_filtered[col] = pd.to_datetime(df_filtered[col], errors='coerce').astype('int64') // 10**9  # Convert to Unix timestamp\n",
    "\n",
    "# One-hot encode all columns, including datetime\n",
    "\n",
    "encoded_df = pd.get_dummies(df_filtered, dummy_na=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>list_date</th>\n",
       "      <th>...</th>\n",
       "      <th>branding_[{'name': None, 'photo': None, 'type': 'Office'}]_True</th>\n",
       "      <th>branding_[{'name': None, 'photo': None, 'type': 'Office'}]_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1704852385</td>\n",
       "      <td>1701483574</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>1704852385</td>\n",
       "      <td>1701483574</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>1704852385</td>\n",
       "      <td>1701483574</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1704852385</td>\n",
       "      <td>1701483574</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>1704852385</td>\n",
       "      <td>1701483574</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5623 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     last_update_date   list_date  ...  \\\n",
       "273        1704852385  1701483574  ...   \n",
       "314        1704852385  1701483574  ...   \n",
       "355        1704852385  1701483574  ...   \n",
       "396        1704852385  1701483574  ...   \n",
       "437        1704852385  1701483574  ...   \n",
       "\n",
       "    branding_[{'name': None, 'photo': None, 'type': 'Office'}]_True  \\\n",
       "273                                              False                \n",
       "314                                              False                \n",
       "355                                              False                \n",
       "396                                              False                \n",
       "437                                              False                \n",
       "\n",
       "    branding_[{'name': None, 'photo': None, 'type': 'Office'}]_nan  \n",
       "273                                              False              \n",
       "314                                              False              \n",
       "355                                              False              \n",
       "396                                              False              \n",
       "437                                              False              \n",
       "\n",
       "[5 rows x 5623 columns]"
      ]
     },
     "execution_count": 1104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the original DataFrame with the encoded DataFrame along the columns axis\n",
    "\n",
    "merged_df = pd.concat([df_filtered, encoded_df], axis=1)\n",
    "\n",
    "# Check the merged dataframe\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sales will vary drastically between cities and states.  Is there a way to keep information about which city it is without OHE such as using central tendency?\n",
    "- Could we label encode or ordinal encode?  Yes, but this may have undesirable effects, giving nominal data ordinal values.\n",
    "- If you replace cities or states with numerical values, make sure that the data is split so that we don't leak data into the training selection. This is a great time to train test split. Compute on the training data, and join these values to the test data\n",
    "- Drop columns that aren't needed.\n",
    "- Don't keep the list price because it will be too close to the sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split here\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = merged_df.drop('description.sold_price', axis=1)  # dropping the 'list_price' column as mentioned\n",
    "y = merged_df['description.sold_price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# do something with state and city\n",
    "# drop any other not needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to csvs\n",
    "\n",
    "X_train.to_csv('x_train.csv', index=False)\n",
    "X_test.to_csv('x_test.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRETCH**\n",
    "\n",
    "- You're not limited to just using the data provided to you. Think/ do some research about other features that might be useful to predict housing prices. \n",
    "- Can you import and join this data? Make sure you do any necessary preprocessing and make sure it is joined correctly.\n",
    "- Example suggestion: could mortgage interest rates in the year of the listing affect the price? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1_1</th>\n",
       "      <th>col1_2</th>\n",
       "      <th>col1_3,4</th>\n",
       "      <th>col1_5</th>\n",
       "      <th>col2_a</th>\n",
       "      <th>col2_b</th>\n",
       "      <th>col2_c</th>\n",
       "      <th>col2_d</th>\n",
       "      <th>col3_1,2</th>\n",
       "      <th>col3_3,4</th>\n",
       "      <th>col3_e</th>\n",
       "      <th>col3_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1_1  col1_2  col1_3,4  col1_5  col2_a  col2_b  col2_c  col2_d  col3_1,2  \\\n",
       "0    True   False     False   False    True   False   False   False      True   \n",
       "1   False    True     False   False   False    True   False   False     False   \n",
       "2   False   False      True   False   False   False    True   False     False   \n",
       "3   False   False     False    True   False   False   False    True     False   \n",
       "\n",
       "   col3_3,4  col3_e  col3_f  \n",
       "0     False   False   False  \n",
       "1     False    True   False  \n",
       "2     False   False    True  \n",
       "3      True   False   False  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import, join and preprocess new data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember all of the EDA that you've been learning about?  Now is a perfect time for it!\n",
    "- Look at distributions of numerical variables to see the shape of the data and detect outliers.\n",
    "- Scatterplots of a numerical variable and the target go a long way to show correlations.\n",
    "- A heatmap will help detect highly correlated features, and we don't want these.\n",
    "- Is there any overlap in any of the features? (redundant information, like number of this or that room...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform EDA here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is a great time to scale the data and save it once it's preprocessed.\n",
    "- You can save it in your data folder, but you may want to make a new `processed/` subfolder to keep it organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
